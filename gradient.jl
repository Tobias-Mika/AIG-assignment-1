lin_reg_grad_descent(X, y, Î±, fit_intercept=true, n_iter=100)

function lin_reg_grad_descent(X, y, Î±, fit_intercept=true, n_iter=100)
# Initialize some useful values
m = length(y) # number of training examples

if fit_intercept
    # Add a constant of 1s if fit_intercept is specified
    constant = ones(m, 1)
    X = hcat(constant, X)
else
    X # Assume user added constants
end

# Use the number of features to initialise the theta Î¸ vector
n = size(X)[2]
Î¸ = zeros(n)

# Initialise the cost vector based on the number of iterations
ğ‰ = zeros(n_iter)

for iter in range(1, stop=n_iter)
    pred = X * Î¸

    # Calcaluate the cost for each iter
    ğ‰[iter] = mean_squared_cost(X, y, Î¸)

    # Update the theta Î¸ at each iter
    Î¸ = Î¸ - ((Î±/m) * X') * (pred - y);
end
return (Î¸, ğ‰)
end
